import os
import json
import logging
import base64
import re
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import trafilatura
from typing import List, Dict, Optional
from sot_guardian import SOTGuardian
from network_guard import NetworkGuard
from total_war_scraper import TotalWarScraper

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

# ì‹¤íŒ¨ ê¸°ì‚¬ ìµœëŒ€ ì¬ì‹œë„ ë¼ìš´ë“œ ìˆ˜
MAX_RETRY_ROUNDS = 3


class GoogleNewsCrawler:
    def __init__(self, sot_path: str = "database/news/news_sot.jsonl", total_war: TotalWarScraper = None):
        self.guardian = SOTGuardian(sot_path)
        self.net_guard = NetworkGuard()
        self.total_war = total_war or TotalWarScraper()

    def _get_headers(self) -> Dict:
        return self.net_guard.get_rotated_headers()

    def decode_url(self, google_url: str) -> str:
        """2-tier Google News URL ë””ì½”ë”©: protobuf íŒŒì‹± â†’ googlenewsdecoder í´ë°±"""
        # Tier A: ì˜¤í”„ë¼ì¸ protobuf íŒŒì‹±
        try:
            match = re.search(r'articles/([^?]+)', google_url)
            if not match:
                return google_url
            encoded = match.group(1)
            padding = len(encoded) % 4
            if padding:
                encoded += '=' * (4 - padding)
            raw = base64.urlsafe_b64decode(encoded)

            # protobuf íŒŒì‹±: ê° í•„ë“œì—ì„œ URL ì¶”ì¶œ
            urls = []
            i = 0
            while i < len(raw):
                if i + 1 >= len(raw):
                    break
                tag = raw[i]
                wire_type = tag & 0x07
                if wire_type == 2:  # length-delimited (string)
                    i += 1
                    length = 0
                    shift = 0
                    while i < len(raw):
                        b = raw[i]
                        length |= (b & 0x7F) << shift
                        shift += 7
                        i += 1
                        if not (b & 0x80):
                            break
                    if i + length <= len(raw):
                        try:
                            field_str = raw[i:i+length].decode('utf-8', errors='ignore')
                            if field_str.startswith('http'):
                                urls.append(field_str)
                        except Exception:
                            pass
                    i += length
                elif wire_type == 0:  # varint
                    i += 1
                    while i < len(raw) and raw[i] & 0x80:
                        i += 1
                    i += 1
                else:
                    break
            if urls:
                result = max(urls, key=len)
                logger.info(f"[Google] Protobuf ë””ì½”ë”© ì„±ê³µ: {result[:60]}...")
                return result
        except Exception:
            pass

        # Tier B: googlenewsdecoder ë¼ì´ë¸ŒëŸ¬ë¦¬ í´ë°±
        try:
            from googlenewsdecoder import new_decoderv1
            decoded = new_decoderv1(google_url, interval=1)
            if decoded and decoded.get("decoded_url"):
                logger.info(f"[Google] googlenewsdecoder ì„±ê³µ: {decoded['decoded_url'][:60]}...")
                return decoded["decoded_url"]
        except Exception as e:
            logger.warning(f"[Google] googlenewsdecoder ì‹¤íŒ¨: {e}")

        return google_url

    def search_news(self, query: str) -> List[Dict]:
        """RSS ê¸°ë°˜ ê²€ìƒ‰ â†’ ì‹¤íŒ¨ ì‹œ ì›¹ í¬ë¡¤ë§ í´ë°±"""
        articles = self._search_via_rss(query)
        if not articles:
            logger.warning("[Google] RSS ìˆ˜ì§‘ ì‹¤íŒ¨ â†’ ì›¹ í¬ë¡¤ë§ í´ë°± ê°€ë™")
            articles = self._search_via_web(query)
        return articles

    def _search_via_rss(self, query: str) -> List[Dict]:
        search_url = f"https://news.google.com/rss/search?q={query}+when:1d&hl=ko&gl=KR&ceid=KR:ko"
        response = self.net_guard.robust_request(search_url, self._get_headers())

        articles = []
        if response:
            try:
                soup = BeautifulSoup(response.text, 'xml')
                for item in soup.select("item"):
                    articles.append({"title": item.title.text, "google_url": item.link.text, "date": item.pubDate.text})
            except Exception as e:
                logger.error(f"[Google] RSS íŒŒì‹± ì‹¤íŒ¨: {e}")
        return articles

    def _search_via_web(self, query: str) -> List[Dict]:
        """RSS ì‹¤íŒ¨ ì‹œ Google News ì›¹ í˜ì´ì§€ ì§ì ‘ í¬ë¡¤ë§"""
        search_url = f"https://news.google.com/search?q={query}+when:1d&hl=ko&gl=KR&ceid=KR:ko"
        tw_result = self.total_war.scrape_with_all_means(search_url)
        if not tw_result:
            # Total Warë„ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ Google ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
            search_url = f"https://www.google.com/search?q={query}&tbm=nws&tbs=qdr:d"
            response = self.net_guard.robust_request(search_url, self._get_headers())
            if not response:
                return []
            html = response.text
        else:
            return []  # Total Warì€ page_sourceë¥¼ ë°˜í™˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì•„ë˜ ë¡œì§ìœ¼ë¡œ ì§„í–‰

        articles = []
        try:
            soup = BeautifulSoup(html, 'lxml')
            for link in soup.select("a[href*='/url?']"):
                href = link.get("href", "")
                url_match = re.search(r'/url\?q=(https?://[^&]+)', href)
                if url_match:
                    real_url = url_match.group(1)
                    title = link.get_text(strip=True) or ""
                    if title and len(title) > 5:
                        articles.append({
                            "title": title,
                            "google_url": real_url,
                            "date": datetime.now().strftime("%Y-%m-%d")
                        })
            logger.info(f"[Google] ì›¹ í¬ë¡¤ë§ í´ë°±ìœ¼ë¡œ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
        except Exception as e:
            logger.error(f"[Google] ì›¹ í¬ë¡¤ë§ í´ë°± íŒŒì‹± ì‹¤íŒ¨: {e}")
        return articles

    def crawl_article(self, info: Dict) -> Optional[Dict]:
        # google_urlì´ ì´ë¯¸ ì‹¤ì œ URLì¸ ê²½ìš° (ì›¹ í¬ë¡¤ë§ í´ë°±) decode ë¶ˆí•„ìš”
        if 'google_url' in info and 'news.google.com' in info['google_url']:
            url = self.decode_url(info['google_url'])
        else:
            url = info.get('google_url', info.get('url', ''))

        # P1: URL ê¸°ë°˜ ì¡°ê¸° ì¤‘ë³µ ê²€ì‚¬ â€” ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì „ì— ì°¨ë‹¨
        if self.guardian.is_url_known(url):
            logger.info(f"[SOT Guardian] URL already in SOT, skipping: {url}")
            return None

        # 1ì°¨ ì‹œë„: í‘œì¤€ ê³ ì† ì¶”ì¶œ
        response = self.net_guard.robust_request(url, self._get_headers())
        content = trafilatura.extract(response.text) if response else None

        # 2ì°¨ ì‹œë„: ì‹¤íŒ¨ ì‹œ Total War ê°€ë™
        if not content or len(content) < 300:
            tw_result = self.total_war.scrape_with_all_means(url)
            if tw_result:
                article = {
                    "title": tw_result['title'], "date": info['date'], "content": tw_result['content'],
                    "url": url, "source": "google", "wf_id": "wf1", "lang": "ko"
                }
                if self.guardian.save_article(article): return article

        if content:
            article = {"title": info['title'], "date": info['date'], "content": content, "url": url, "source": "google", "wf_id": "wf1", "lang": "ko"}
            if self.guardian.save_article(article): return article

        logger.error(f"âŒ [MISSION FAIL] Google ìˆ˜ì§‘ ì‹¤íŒ¨ (ì¬ì‹œë„ ëŒ€ìƒ): {url}")
        return None

    def run(self, query: str):
        articles = self.search_news(query)
        logger.info(f"[Google] ë°œê²¬ëœ ê¸°ì‚¬: {len(articles)}ê°œ")

        # 1ì°¨ ìˆ˜ì§‘
        failed_infos = []
        for info in articles:
            result = self.crawl_article(info)
            url = self.decode_url(info.get('google_url', ''))
            if result is None and not self.guardian.is_url_known(url):
                failed_infos.append(info)

        # ì‹¤íŒ¨ ê¸°ì‚¬ ì¬ì‹œë„ (ìµœëŒ€ MAX_RETRY_ROUNDS ë¼ìš´ë“œ)
        for round_num in range(1, MAX_RETRY_ROUNDS + 1):
            if not failed_infos:
                break
            logger.warning(f"ğŸ”„ [Google] ì¬ì‹œë„ ë¼ìš´ë“œ {round_num}/{MAX_RETRY_ROUNDS}: {len(failed_infos)}ê°œ ì‹¤íŒ¨ ê¸°ì‚¬")
            time.sleep(5 * round_num)
            still_failed = []
            for info in failed_infos:
                result = self.crawl_article(info)
                url = self.decode_url(info.get('google_url', ''))
                if result is None and not self.guardian.is_url_known(url):
                    still_failed.append(info)
            failed_infos = still_failed

        if failed_infos:
            logger.error(f"âš ï¸ [Google] ìµœì¢… ë¯¸ìˆ˜ì§‘ ê¸°ì‚¬: {len(failed_infos)}ê°œ")
        else:
            logger.info(f"âœ… [Google] ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
