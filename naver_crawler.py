import os
import json
import time
import requests
import logging
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from sot_guardian import SOTGuardian
from network_guard import NetworkGuard
from total_war_scraper import TotalWarScraper

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

# ì‹¤íŒ¨ ê¸°ì‚¬ ìµœëŒ€ ì¬ì‹œë„ ë¼ìš´ë“œ ìˆ˜
MAX_RETRY_ROUNDS = 3


class NaverNewsCrawler:
    def __init__(self, sot_path: str = "database/news/news_sot.jsonl", total_war: TotalWarScraper = None):
        self.guardian = SOTGuardian(sot_path)
        self.net_guard = NetworkGuard()
        self.total_war = total_war or TotalWarScraper()
        self.session = requests.Session()

    def _get_headers(self) -> Dict:
        return self.net_guard.get_rotated_headers()

    def search_news(self, query: str) -> List[str]:
        urls = []
        page = 0
        consecutive_empty = 0
        while page < 10:  # ìµœëŒ€ 100ê°œ ê¸°ì‚¬ (ì¼ê°„ ìŠ¤ìº”ì— ì¶©ë¶„)
            start = page * 10 + 1
            search_url = f"https://search.naver.com/search.naver?where=news&query={query}&pd=1&start={start}"

            # 7ëŒ€ ì›ì¹™ ì ìš©ëœ ìš”ì²­
            response = self.net_guard.robust_request(search_url, self._get_headers())
            if not response: break

            try:
                # 5. íŒŒì‹± ì˜¤ë¥˜ ê²€ì‚¬
                soup = BeautifulSoup(response.text, 'lxml')
                links = soup.select("a")
                found_new = False
                for link in links:
                    href = link.get('href', '')
                    if "n.news.naver.com/mnews/article" in href:
                        clean_url = href.split("?")[0]
                        if clean_url not in urls:
                            urls.append(clean_url)
                            found_new = True
                if not found_new:
                    consecutive_empty += 1
                    if consecutive_empty >= 2:
                        logger.info(f"[Naver] ì—°ì† {consecutive_empty}í˜ì´ì§€ ì‹ ê·œ ê¸°ì‚¬ ì—†ìŒ â†’ ì¡°ê¸° ì¢…ë£Œ")
                        break
                else:
                    consecutive_empty = 0
                page += 1
            except Exception as e:
                logger.error(f"[Naver] 5. íŒŒì‹± ì‹¤íŒ¨: {e}")
                break

        return list(set(urls))

    def crawl_article(self, url: str) -> Optional[Dict]:
        # P1: URL ê¸°ë°˜ ì¡°ê¸° ì¤‘ë³µ ê²€ì‚¬ â€” ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì „ì— ì°¨ë‹¨
        if self.guardian.is_url_known(url):
            logger.info(f"[SOT Guardian] URL already in SOT, skipping: {url}")
            return None

        # 1-4ë‹¨ê³„ ë° 6ë‹¨ê³„ ì›ì¹™ ì ìš©
        response = self.net_guard.robust_request(url, self._get_headers())

        article_data = None
        if response:
            try:
                soup = BeautifulSoup(response.text, 'lxml')
                title_elem = soup.select_one("#title_area span, .media_end_head_headline")
                title = title_elem.get_text(strip=True) if title_elem else ""

                date_elem = soup.select_one(".media_end_head_info_datestamp_time, .t11")
                date_str = date_elem.get_text(strip=True) if date_elem else ""

                content_elem = soup.select_one("#dic_area, #newsct_article")
                if content_elem:
                    for unwanted in content_elem.select(".article_footer, .img_desc, script, style"): unwanted.decompose()
                    content = content_elem.get_text(strip=True)
                    if len(content) > 200:
                        article_data = {"title": title, "date": date_str, "content": content}
            except: pass

        # [ì ˆëŒ€ ê¸°ì¤€] ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ Total War ê°€ë™
        if not article_data:
            tw_result = self.total_war.scrape_with_all_means(url)
            if tw_result:
                article_data = {
                    "title": tw_result['title'],
                    "date": datetime.now().strftime("%Y-%m-%d"),  # ì¼ì ëˆ„ë½ ì‹œ ì˜¤ëŠ˜ ë‚ ì§œ
                    "content": tw_result['content']
                }

        if article_data:
            article = {**article_data, "url": url, "source": "naver", "wf_id": "wf1", "lang": "ko"}
            if self.guardian.save_article(article):
                return article

        logger.error(f"âŒ [MISSION FAIL] Naver ìˆ˜ì§‘ ì‹¤íŒ¨ (ì¬ì‹œë„ ëŒ€ìƒ): {url}")
        return None

    def run(self, query: str):
        urls = self.search_news(query)
        logger.info(f"[Naver] ë°œê²¬ëœ ê¸°ì‚¬ URL: {len(urls)}ê°œ")

        # 1ì°¨ ìˆ˜ì§‘
        failed_urls = []
        for url in urls:
            result = self.crawl_article(url)
            if result is None and not self.guardian.is_url_known(url):
                failed_urls.append(url)

        # ì‹¤íŒ¨ ê¸°ì‚¬ ì¬ì‹œë„ (ìµœëŒ€ MAX_RETRY_ROUNDS ë¼ìš´ë“œ)
        for round_num in range(1, MAX_RETRY_ROUNDS + 1):
            if not failed_urls:
                break
            logger.warning(f"ğŸ”„ [Naver] ì¬ì‹œë„ ë¼ìš´ë“œ {round_num}/{MAX_RETRY_ROUNDS}: {len(failed_urls)}ê°œ ì‹¤íŒ¨ ê¸°ì‚¬")
            time.sleep(5 * round_num)  # ë¼ìš´ë“œë§ˆë‹¤ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
            still_failed = []
            for url in failed_urls:
                result = self.crawl_article(url)
                if result is None and not self.guardian.is_url_known(url):
                    still_failed.append(url)
            failed_urls = still_failed

        if failed_urls:
            logger.error(f"âš ï¸ [Naver] ìµœì¢… ë¯¸ìˆ˜ì§‘ ê¸°ì‚¬: {len(failed_urls)}ê°œ")
        else:
            logger.info(f"âœ… [Naver] ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
